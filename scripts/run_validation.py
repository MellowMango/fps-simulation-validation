#!/usr/bin/env python3
"""
Main FPS validation script.
Runs full FPS + control comparison and exits 0 only if all asserts pass.
"""

import sys
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
import json
import time
import hashlib

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from fps.dynamics import FPSDynamics
from fps.metrics import FPSMetrics
from fps.config import create_default_config, create_kuramoto_config, save_config
from fps.kuramoto import run_kuramoto_control


def save_results_csv(results: dict, filepath: str, model_type: str = "fps"):
    """Save simulation results to CSV."""
    if model_type == "fps":
        df = pd.DataFrame({
            't': results['time'],
            'S': results['S'],
            'C': results['C'],
            'r': results['r']
        })
    else:  # kuramoto
        df = pd.DataFrame({
            't': results['time'],
            'r': results['r'],
            'psi': results['psi'],
            'mean_theta': np.mean(results['theta'], axis=1)
        })
    df.to_csv(filepath, index=False)
    print(f"Results saved to {filepath}")


def create_plots(fps_results: dict, kuramoto_results: dict, output_dir: str):
    """Create comparison plots."""
    os.makedirs(output_dir, exist_ok=True)
    
    # FPS plots
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    
    # Global signal S(t)
    axes[0, 0].plot(fps_results['time'], fps_results['S'])
    axes[0, 0].set_title('FPS Global Signal S(t)')
    axes[0, 0].set_xlabel('Time')
    axes[0, 0].set_ylabel('S(t)')
    
    # Coherence C(t)
    axes[0, 1].plot(fps_results['time'], fps_results['C'])
    axes[0, 1].set_title('FPS Coherence C(t)')
    axes[0, 1].set_xlabel('Time')
    axes[0, 1].set_ylabel('C(t)')
    
    # Spiral ratio r(t)
    axes[1, 0].plot(fps_results['time'], fps_results['r'])
    axes[1, 0].axhline(y=1.618, color='r', linestyle='--', label='Golden ratio')
    axes[1, 0].set_title('FPS Spiral Ratio r(t)')
    axes[1, 0].set_xlabel('Time')
    axes[1, 0].set_ylabel('r(t)')
    axes[1, 0].legend()
    
    # Kuramoto order parameter
    axes[1, 1].plot(kuramoto_results['time'], kuramoto_results['r'])
    axes[1, 1].set_title('Kuramoto Order Parameter r(t)')
    axes[1, 1].set_xlabel('Time')
    axes[1, 1].set_ylabel('r(t)')
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}/comparison_plots.png', dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"Plots saved to {output_dir}/comparison_plots.png")


def generate_comparison_report(fps_results: dict, kuramoto_results: dict, 
                             fps_metrics: dict, output_dir: str):
    """Generate markdown comparison report."""
    
    # CPU comparison
    fps_cpu = np.mean(fps_results['cpu_times'])
    kuramoto_cpu = np.mean(kuramoto_results['cpu_times'])
    cpu_ratio = fps_cpu / kuramoto_cpu if kuramoto_cpu > 0 else float('inf')
    
    report = f"""# FPS vs Kuramoto Comparison Report

## Executive Summary
- **FPS CPU Efficiency**: {cpu_ratio:.2f}x vs Kuramoto {'✓ PASS' if cpu_ratio < 2.0 else '✗ FAIL'}
- **Overall Validation**: {'✓ ALL PASS' if fps_metrics['all_passed'] else '✗ SOME FAILED'}

## Performance Metrics

### CPU Performance
- FPS mean CPU per step: {fps_cpu:.6f}s
- Kuramoto mean CPU per step: {kuramoto_cpu:.6f}s
- **Ratio (FPS/Kuramoto)**: {cpu_ratio:.2f}x
- **Threshold**: < 2.0x ({'✓ PASS' if cpu_ratio < 2.0 else '✗ FAIL'})

### FPS Validation Results
"""
    
    # Add validation results
    for metric_name, (value, passed) in fps_metrics['metrics'].items():
        status = '✓ PASS' if passed else '✗ FAIL'
        report += f"- **{metric_name.title()}**: {value:.6f} ({status})\n"
    
    report += f"""
### Summary Statistics
- **Pass Rate**: {fps_metrics['summary']['pass_rate']:.1%}
- **Passed Metrics**: {fps_metrics['summary']['passed']}/{fps_metrics['summary']['total']}
- **Failed Metrics**: {fps_metrics['summary']['failed_metrics']}

## Simulation Parameters
- **FPS Strates (N)**: {fps_results['config']['system']['N']}
- **Kuramoto Oscillators (N)**: {kuramoto_results['config']['N']}
- **Simulation Time**: {fps_results['config']['system']['T']}s
- **Time Step**: {fps_results['config']['system']['dt']}s
- **Random Seed**: {fps_results['config']['system']['seed']}

## Key Findings

### FPS Advantages
- Spiral ratio convergence to golden ratio (φ = 1.618)
- Rich dynamics with controlled coherence
- {'Efficient computation vs Kuramoto' if cpu_ratio < 2.0 else 'Higher computational cost than expected'}

### Validation Status
{'All validation criteria PASSED - FPS theory holds!' if fps_metrics['all_passed'] else 'Some validation criteria FAILED - theory needs refinement.'}

---
*Generated by FPS validation pipeline*
"""
    
    report_path = f'{output_dir}/comparison_report.md'
    with open(report_path, 'w') as f:
        f.write(report)
    
    print(f"Comparison report saved to {report_path}")
    return report_path


def compute_file_hash(filepath: str) -> str:
    """Compute SHA-256 hash of file."""
    sha256_hash = hashlib.sha256()
    with open(filepath, "rb") as f:
        for byte_block in iter(lambda: f.read(4096), b""):
            sha256_hash.update(byte_block)
    return sha256_hash.hexdigest()


def run_golden_test():
    """Run the golden reference test for reproducibility."""
    print("Running golden reference test...")
    
    # Golden run: N=5, T=20, seed=42, scenario #1 (constant)
    config = create_default_config(N=5, T=20.0, seed=42)
    
    # Run FPS simulation
    fps_model = FPSDynamics(config)
    results = fps_model.run_simulation(input_scenario="constant", use_extended=False)
    
    # Save golden files
    os.makedirs('validation_artifacts/golden', exist_ok=True)
    
    # Save CSV
    csv_path = 'validation_artifacts/golden/golden_run.csv'
    save_results_csv(results, csv_path)
    
    # Save PNG plot
    plt.figure(figsize=(10, 6))
    plt.subplot(1, 3, 1)
    plt.plot(results['time'], results['S'])
    plt.title('S(t)')
    plt.xlabel('Time')
    
    plt.subplot(1, 3, 2)
    plt.plot(results['time'], results['C'])
    plt.title('C(t)')
    plt.xlabel('Time')
    
    plt.subplot(1, 3, 3)
    plt.plot(results['time'], results['r'])
    plt.axhline(y=1.618, color='r', linestyle='--')
    plt.title('r(t)')
    plt.xlabel('Time')
    
    plt.tight_layout()
    png_path = 'validation_artifacts/golden/golden_run.png'
    plt.savefig(png_path, dpi=150)
    plt.close()
    
    # Compute and save hashes for validation
    csv_hash = compute_file_hash(csv_path)
    png_hash = compute_file_hash(png_path)
    
    hash_record = {
        'csv_hash': csv_hash,
        'png_hash': png_hash,
        'config': config,
        'timestamp': time.time()
    }
    
    hash_path = 'validation_artifacts/golden/golden_hashes.json'
    with open(hash_path, 'w') as f:
        json.dump(hash_record, f, indent=2, default=str)
    
    print(f"Golden reference test completed!")
    print(f"CSV hash: {csv_hash}")
    print(f"PNG hash: {png_hash}")
    
    # Validate against reference if exists
    reference_path = 'validation_artifacts/golden/reference_hashes.json'
    if os.path.exists(reference_path):
        with open(reference_path, 'r') as f:
            reference = json.load(f)
        
        if csv_hash == reference['csv_hash'] and png_hash == reference['png_hash']:
            print("✓ GOLDEN TEST PASSED - Byte-for-byte match with reference")
            return True
        else:
            print("✗ GOLDEN TEST FAILED - Hash mismatch with reference")
            print(f"Expected CSV: {reference['csv_hash']}")
            print(f"Expected PNG: {reference['png_hash']}")
            return False
    else:
        print("⚠️  No reference hashes found - this run becomes the reference")
        # Copy current hashes as reference
        import shutil
        shutil.copy(hash_path, reference_path)
        return True
    
    return results


def main():
    """Main validation pipeline."""
    print("=== FPS Validation Pipeline ===")
    print("Running comprehensive FPS + Kuramoto comparison...")
    
    # Create output directory
    os.makedirs('validation_artifacts', exist_ok=True)
    
    # Step 1: Create configurations
    print("\n1. Creating configurations...")
    fps_config = create_default_config(N=20, T=20.0, seed=123)
    kuramoto_config = create_kuramoto_config(N=20, seed=123)
    
    save_config(fps_config, 'validation_artifacts/fps_config.json')
    save_config(kuramoto_config, 'validation_artifacts/kuramoto_config.json')
    
    # Step 2: Run FPS simulation
    print("\n2. Running FPS simulation...")
    start_time = time.time()
    fps_model = FPSDynamics(fps_config)
    fps_results = fps_model.run_simulation(input_scenario="constant", use_extended=False)
    fps_duration = time.time() - start_time
    print(f"   FPS simulation completed in {fps_duration:.2f}s")
    
    # Step 3: Run Kuramoto control
    print("\n3. Running Kuramoto control simulation...")
    start_time = time.time()
    kuramoto_results = run_kuramoto_control(kuramoto_config)
    kuramoto_duration = time.time() - start_time
    print(f"   Kuramoto simulation completed in {kuramoto_duration:.2f}s")
    
    # Step 4: Run validation metrics
    print("\n4. Running validation metrics...")
    fps_metrics_engine = FPSMetrics(fps_results)
    kuramoto_cpu_time = np.mean(kuramoto_results['cpu_times'])
    
    fps_validation = fps_metrics_engine.run_all_validations(
        perturbation_log=None,
        kuramoto_cpu_time=kuramoto_cpu_time
    )
    
    # Step 5: Generate outputs
    print("\n5. Generating outputs...")
    save_results_csv(fps_results, 'validation_artifacts/fps_results.csv')
    save_results_csv(kuramoto_results, 'validation_artifacts/kuramoto_results.csv', model_type="kuramoto")
    
    create_plots(fps_results, kuramoto_results, 'validation_artifacts')
    report_path = generate_comparison_report(fps_results, kuramoto_results, 
                                           fps_validation, 'validation_artifacts')
    
    # Step 6: Golden test
    print("\n6. Running golden reference test...")
    golden_results = run_golden_test()
    
    # Step 7: Assert all criteria
    print("\n7. Final validation check...")
    try:
        fps_metrics_engine.assert_all_criteria(
            perturbation_log=None,
            kuramoto_cpu_time=kuramoto_cpu_time
        )
        print("✓ ALL VALIDATION CRITERIA PASSED!")
        print("\nFPS theory validated successfully!")
        print(f"- Pass rate: {fps_validation['summary']['pass_rate']:.1%}")
        print(f"- CPU efficiency: {np.mean(fps_results['cpu_times'])/kuramoto_cpu_time:.2f}x vs Kuramoto")
        print(f"- Report: {report_path}")
        
        return 0  # Success exit code
        
    except AssertionError as e:
        print(f"✗ VALIDATION FAILED: {e}")
        print("\nFPS theory requires refinement!")
        print(f"- Pass rate: {fps_validation['summary']['pass_rate']:.1%}")
        print(f"- Failed metrics: {fps_validation['summary']['failed_metrics']}")
        print(f"- See criteria_failures.csv for details")
        
        return 1  # Failure exit code


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code) 